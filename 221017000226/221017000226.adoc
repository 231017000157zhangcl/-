= What's this
:toc: manual

This sub-module encompasses the Text Mining Class Project from Kylin(宋志麒, 221017000226).

Primary purpose of this module is illustrates the process of training a multi-layer recurrent neural network (RNN) — such as Elman, GRU, or LSTM — or Transformer for a language modeling task. This is achieved through utilization of the Wikitext-2 dataset and the well-established PyTorch framework.

== wikitext-2 raw dataset

Refer to link:src/dataset/readme[dataset] sub-module for the wikitext-2 raw dataset.

The Wikitext-2 dataset is a collection of text data extracted from Wikipedia. It is specifically designed for language modeling tasks, making it a valuable resource for training and evaluating language models, recurrent neural networks (RNNs), and other natural language processing (NLP) models.

Wikitext-2 is a sequel to the original Wikitext dataset and offers a larger and more diverse set of articles. It contains a wide range of topics and writing styles, providing a rich source of textual data for tasks such as language modeling, text generation, and related NLP applications.

== Word-level Language Modeling

=== PlaceHolder Two

=== PlaceHolder Three

=== PlaceHolder Four

== torch functions

[cols="1,2,4"]
|===
|Function |Note |Parameters

|torch.manual_seed(seed)
|Sets the seed for generating random numbers. Returns a torch.Generator object.
|
* seed (int) – The desired seed. Value must be within the inclusive range [-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError is raised. Negative inputs are remapped to positive values with the formula 0xffff_ffff_ffff_ffff + seed.

|===

